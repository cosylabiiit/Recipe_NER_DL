{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e84e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6e945",
   "metadata": {},
   "source": [
    "***Synonym replacement***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d3fd8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('final_train.csv')\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Function to replace entity with synonyms\n",
    "def replace_entity_with_synonym(entity):\n",
    "    if entity.isnumeric():\n",
    "        # Replace number with a random number\n",
    "        temp=entity*2\n",
    "        return temp\n",
    "    else:\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(entity):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().lower())\n",
    "\n",
    "        if len(synonyms) == 0:\n",
    "            return entity\n",
    "\n",
    "        # Replace entity with random synonym\n",
    "        synonym = random.choice(list(synonyms))\n",
    "        return synonym\n",
    "\n",
    "# Function to perform data augmentation using synonym replacement\n",
    "def augment_dataset_with_synonyms(dataset):\n",
    "    augmented_dataset = []\n",
    "\n",
    "    for sentence, entities in dataset:\n",
    "        augmented_entities = []\n",
    "        for entity, entity_type in entities:\n",
    "            # Replace entity with synonym or random number\n",
    "            new_entity = replace_entity_with_synonym(entity)\n",
    "\n",
    "            # Append original entity and new entity to list\n",
    "            #augmented_entities.append((entity, entity_type))\n",
    "            augmented_entities.append((new_entity, entity_type))\n",
    "\n",
    "        # Append original sentence and augmented entities to dataset\n",
    "        augmented_dataset.append((sentence, augmented_entities))\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "dataset = []\n",
    "for sentence, tags in zip(df_train.sentence, df_train.tags):\n",
    "    temp3 = []\n",
    "    for token, entity in zip(sentence.split(), tags.split()):\n",
    "        temp3.append((token, entity))\n",
    "    dataset.append((sentence,temp3))\n",
    "\n",
    "augmented_dataset = augment_dataset_with_synonyms(dataset)\n",
    "\n",
    "train_aug_sent=[]\n",
    "train_aug_tag=[]\n",
    "for sentence, entities in augmented_dataset:\n",
    "    temp_sen=''\n",
    "    temp_tag=''\n",
    "    for i in entities:\n",
    "        temp_sen+=i[0]\n",
    "        temp_tag+=i[1]\n",
    "        temp_sen+=' '\n",
    "        temp_tag+=' '\n",
    "    temp_sen=temp_sen.strip()\n",
    "    temp_tag=temp_tag.strip()\n",
    "    train_aug_sent.append(temp_sen)\n",
    "    train_aug_tag.append(temp_tag)\n",
    "    \n",
    "pd1=pd.DataFrame(list(zip(train_aug_sent, train_aug_tag)),columns =['sentence','tags'])\n",
    "pd1.to_csv('final_train_sr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2510ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('final_test.csv')\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Function to replace entity with synonyms\n",
    "def replace_entity_with_synonym(entity):\n",
    "    if entity.isnumeric():\n",
    "        # Replace number with a random number\n",
    "        temp=entity*2\n",
    "        return temp\n",
    "    else:\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(entity):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.add(lemma.name().lower())\n",
    "\n",
    "        if len(synonyms) == 0:\n",
    "            return entity\n",
    "\n",
    "        # Replace entity with random synonym\n",
    "        synonym = random.choice(list(synonyms))\n",
    "        return synonym\n",
    "\n",
    "# Function to perform data augmentation using synonym replacement\n",
    "def augment_dataset_with_synonyms(dataset):\n",
    "    augmented_dataset = []\n",
    "\n",
    "    for sentence, entities in dataset:\n",
    "        augmented_entities = []\n",
    "        for entity, entity_type in entities:\n",
    "            # Replace entity with synonym or random number\n",
    "            new_entity = replace_entity_with_synonym(entity)\n",
    "\n",
    "            # Append original entity and new entity to list\n",
    "            #augmented_entities.append((entity, entity_type))\n",
    "            augmented_entities.append((new_entity, entity_type))\n",
    "\n",
    "        # Append original sentence and augmented entities to dataset\n",
    "        augmented_dataset.append((sentence, augmented_entities))\n",
    "\n",
    "    return augmented_dataset\n",
    "\n",
    "dataset = []\n",
    "for sentence, tags in zip(df_train.sentence, df_train.tags):\n",
    "    temp3 = []\n",
    "    for token, entity in zip(sentence.split(), tags.split()):\n",
    "        temp3.append((token, entity))\n",
    "    dataset.append((sentence,temp3))\n",
    "\n",
    "augmented_dataset = augment_dataset_with_synonyms(dataset)\n",
    "\n",
    "train_aug_sent=[]\n",
    "train_aug_tag=[]\n",
    "for sentence, entities in augmented_dataset:\n",
    "    temp_sen=''\n",
    "    temp_tag=''\n",
    "    for i in entities:\n",
    "        temp_sen+=i[0]\n",
    "        temp_tag+=i[1]\n",
    "        temp_sen+=' '\n",
    "        temp_tag+=' '\n",
    "    temp_sen=temp_sen.strip()\n",
    "    temp_tag=temp_tag.strip()\n",
    "    train_aug_sent.append(temp_sen)\n",
    "    train_aug_tag.append(temp_tag)\n",
    "    \n",
    "pd1=pd.DataFrame(list(zip(train_aug_sent, train_aug_tag)),columns =['sentence','tags'])\n",
    "pd1.to_csv('final_test_sr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7df02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea6f5863",
   "metadata": {},
   "source": [
    "***Label Wise***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ddbe618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "df_train=pd.read_csv('final_train.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "# Function to replace entity with random tokenwise\n",
    "def label_wise_token_replacement_augmentation(tag, label_map):\n",
    "    new_entity = random.choice(label_map[tag])\n",
    "    return new_entity\n",
    "\n",
    "\n",
    "# Function to perform data augmentation using random token replacement\n",
    "def augment_dataset(dataset):\n",
    "    augmented_dataset = []\n",
    "\n",
    "    for sentence, entities in dataset:\n",
    "        augmented_entities = []\n",
    "        for entity, entity_type in entities:\n",
    "            new_entity = label_wise_token_replacement_augmentation(entity_type, label_map)\n",
    "            \n",
    "            # Append original entity and new entity to list\n",
    "            augmented_entities.append((new_entity, entity_type))\n",
    "\n",
    "        # Append original sentence and augmented entities to dataset\n",
    "        augmented_dataset.append((sentence, augmented_entities))\n",
    "    return augmented_dataset\n",
    "\n",
    "\n",
    "\n",
    "# create dataset from dataframe\n",
    "dataset = []\n",
    "for sentence, tags in zip(df_train.sentence, df_train.tags):\n",
    "  temp = []\n",
    "  for token, entity in zip(sentence.split(), tags.split()):\n",
    "    temp.append((token, entity))\n",
    "  dataset.append((sentence, temp))\n",
    "\n",
    "\n",
    "# create empty label map for entity and tags match\n",
    "label_map = {\"UNIT\": [], \"O\": [], \"STATE\": [], \"QUANTITY\": [], \"NAME\": [], \"SIZE\": [], \"TEMP\": [], \"DF\": []}\n",
    "\n",
    "\n",
    "# fill label map from dataset\n",
    "for sentence, tags_pair in dataset:\n",
    "  for entity, tag in tags_pair:\n",
    "      label_map[tag].append(entity)\n",
    "\n",
    "\n",
    "# generate augmented dataset\n",
    "augmented_dataset = augment_dataset(dataset)\n",
    "\n",
    "train_aug_sent=[]\n",
    "train_aug_tag=[]\n",
    "for sentence, entities in augmented_dataset:\n",
    "    temp_sen=''\n",
    "    temp_tag=''\n",
    "    for i in entities:\n",
    "        temp_sen+=i[0]\n",
    "        temp_tag+=i[1]\n",
    "        temp_sen+=' '\n",
    "        temp_tag+=' '\n",
    "    temp_sen=temp_sen.strip()\n",
    "    temp_tag=temp_tag.strip()\n",
    "    train_aug_sent.append(temp_sen)\n",
    "    train_aug_tag.append(temp_tag)\n",
    "    \n",
    "pd1=pd.DataFrame(list(zip(train_aug_sent, train_aug_tag)),columns =['sentence','tags'])\n",
    "pd1.to_csv('final_train_label_wise.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fd0284fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "df_train=pd.read_csv('final_test.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "# Function to replace entity with random tokenwise\n",
    "def label_wise_token_replacement_augmentation(tag, label_map):\n",
    "    new_entity = random.choice(label_map[tag])\n",
    "    return new_entity\n",
    "\n",
    "\n",
    "# Function to perform data augmentation using random token replacement\n",
    "def augment_dataset(dataset):\n",
    "    augmented_dataset = []\n",
    "\n",
    "    for sentence, entities in dataset:\n",
    "        augmented_entities = []\n",
    "        for entity, entity_type in entities:\n",
    "            new_entity = label_wise_token_replacement_augmentation(entity_type, label_map)\n",
    "            \n",
    "            # Append original entity and new entity to list\n",
    "            augmented_entities.append((new_entity, entity_type))\n",
    "\n",
    "        # Append original sentence and augmented entities to dataset\n",
    "        augmented_dataset.append((sentence, augmented_entities))\n",
    "    return augmented_dataset\n",
    "\n",
    "\n",
    "\n",
    "# create dataset from dataframe\n",
    "dataset = []\n",
    "for sentence, tags in zip(df_train.sentence, df_train.tags):\n",
    "  temp = []\n",
    "  for token, entity in zip(sentence.split(), tags.split()):\n",
    "    temp.append((token, entity))\n",
    "  dataset.append((sentence, temp))\n",
    "\n",
    "\n",
    "# create empty label map for entity and tags match\n",
    "label_map = {\"UNIT\": [], \"O\": [], \"STATE\": [], \"QUANTITY\": [], \"NAME\": [], \"SIZE\": [], \"TEMP\": [], \"DF\": []}\n",
    "\n",
    "\n",
    "# fill label map from dataset\n",
    "for sentence, tags_pair in dataset:\n",
    "  for entity, tag in tags_pair:\n",
    "      label_map[tag].append(entity)\n",
    "\n",
    "\n",
    "# generate augmented dataset\n",
    "augmented_dataset = augment_dataset(dataset)\n",
    "\n",
    "train_aug_sent=[]\n",
    "train_aug_tag=[]\n",
    "for sentence, entities in augmented_dataset:\n",
    "    temp_sen=''\n",
    "    temp_tag=''\n",
    "    for i in entities:\n",
    "        temp_sen+=i[0]\n",
    "        temp_tag+=i[1]\n",
    "        temp_sen+=' '\n",
    "        temp_tag+=' '\n",
    "    temp_sen=temp_sen.strip()\n",
    "    temp_tag=temp_tag.strip()\n",
    "    train_aug_sent.append(temp_sen)\n",
    "    train_aug_tag.append(temp_tag)\n",
    "    \n",
    "pd1=pd.DataFrame(list(zip(train_aug_sent, train_aug_tag)),columns =['sentence','tags'])\n",
    "pd1.to_csv('final_test_label_wise.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e85f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "497857a1",
   "metadata": {},
   "source": [
    "***Shuffle with segments***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "6e97edcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('final_train.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "train_sent=[]\n",
    "train_tag=[]\n",
    "for i in df_train['sentence']:\n",
    "    train_sent.append(i)\n",
    "for i in df_train['tags']:\n",
    "    train_tag.append(i)\n",
    "\n",
    "train_sent_list=[]\n",
    "train_tag_list=[]\n",
    "for i in range(len(train_sent)):\n",
    "    temp_sent=(train_sent[i].split(\" \"))\n",
    "    temp_tag=(train_tag[i].split(\" \"))\n",
    "    train_sent_list.append(temp_sent)\n",
    "    train_tag_list.append(temp_tag)\n",
    "    \n",
    "# create dataset from dataframe\n",
    "dataset = []\n",
    "for sentence, tags in zip(df_train.sentence, df_train.tags):\n",
    "    temp = []\n",
    "    for token, entity in zip(sentence.split(), tags.split()):\n",
    "        temp.append((token, entity))\n",
    "    dataset.append((sentence, temp))\n",
    "\n",
    "def shuffle_with_segments(entities):\n",
    "    label_map = {}\n",
    "    for entity, tags in entities:\n",
    "        if tags not in label_map:\n",
    "            label_map[tags] = [entity]\n",
    "        else:\n",
    "            label_map[tags].append(entity)\n",
    "    for key in label_map:\n",
    "        random.shuffle(label_map[key])\n",
    "    return label_map\n",
    "\n",
    "dataset2=[]\n",
    "for i,j in dataset:\n",
    "    temp2=shuffle_with_segments(j)\n",
    "    dataset2.append(temp2)\n",
    "\n",
    "aug_train_set=[]\n",
    "for i in range(len(train_sent_list)):\n",
    "    try:\n",
    "        temp=[]\n",
    "        count=0\n",
    "        for j in train_tag_list[i]:\n",
    "            if(len(dataset2[i][j])==1):\n",
    "                temp.append(train_sent_list[i][count])\n",
    "            else:\n",
    "                val=random.choice(dataset2[i][j])\n",
    "                temp.append(val)\n",
    "                dataset2[i][j].remove(val)\n",
    "            count=count+1\n",
    "        aug_train_set.append(temp)\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "aug_train_sent=[]\n",
    "for i in aug_train_set:\n",
    "    temp = ' '.join([str(elem) for elem in i])\n",
    "    aug_train_sent.append(temp)\n",
    "\n",
    "pd1=pd.DataFrame(list(zip(train_aug_sent, train_aug_tag)),columns =['sentence','tags'])\n",
    "pd1.to_csv('final_train_sis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0cfa2723",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train=pd.read_csv('final_train.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "final_train_label_wise=pd.read_csv('final_train_label_wise.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "final_train_sr=pd.read_csv('final_train_sr.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "final_train_sis=pd.read_csv('final_train_sis.csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "final_pd=pd.concat([final_train, final_train_label_wise, final_train_sr, final_train_sis ])\n",
    "final_pd.to_csv('final_train_augmented.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3da643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
